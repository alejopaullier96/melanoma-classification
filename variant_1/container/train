#!/usr/bin/env python

# This implementation works in File mode.

from __future__ import print_function

import json
import os
import shutil
import sys
import traceback

# Basics
import pandas as pd
import torch
import warnings
warnings.filterwarnings("ignore")

# Local utility code
from config import config, paths
from dataset.dataset import MelanomaDataset
from models.efficientnet.hyperparameters import hyperparameters as ef_hp
from models.efficientnet.model import EfficientNetwork
from preprocessing import preprocess
from train_function import train_function
import utils


# These are the paths to where SageMaker mounts interesting things in your container.
prefix = '/opt/ml/'
input_path = prefix + 'input/data'
output_path = os.path.join(prefix, 'output')
model_path = os.path.join(prefix, 'model')
param_path = os.path.join(prefix, 'input/config/hyperparameters.json')

# This algorithm has a single channel of input data called 'training'. Since we run in
# File mode, the input files are copied to the directory specified here.
channel_name='training'
training_path = os.path.join(input_path, channel_name)

utils.set_seed()
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print('Device available now: ', device)

# The function to execute the training.
def train():
    print('Starting the training.')
    try:
        # Read in any hyperparameters that the user passed with the training job
        with open(param_path, 'r') as tc:
            trainingParams = json.load(tc)

        # Take the set of files and read them all into a single pandas dataframe
        input_files = [ os.path.join(training_path, file) for file in os.listdir(training_path) ]
        if len(input_files) == 0:
            raise ValueError(('There are no files in {}.\n' +
                              'This usually indicates that the channel ({}) was incorrectly specified,\n' +
                              'the data specification in S3 was incorrectly specified or the role specified\n' +
                              'does not have permission to access the data.').format(training_path, channel_name))

        data_location = "/opt/ml/input/data/training/"
        # Preprocessing
        train_df = pd.read_csv(data_location+"train.csv", sep=',')
        test_df = pd.read_csv(data_location+"test.csv", sep=',')
        train_df = utils.add_path_column(train_df, "image_id", "path_jpg", data_location+"train_jpg/")
        test_df = utils.add_path_column(test_df, "image_id", "path_jpg", data_location+"test_jpg/")
        train_df, label_encoders = preprocess.label_encode(train_df)
        test_df = preprocess.label_encode_transform(test_df, label_encoders)

        # Create folds
        folds = utils.create_folds(train_df, config.FOLDS)

        # Predictions
        predictions = torch.zeros(size=(len(test_df), 1), dtype=torch.float32, device=device)

        # Create model instance
        model = EfficientNetwork(output_size=config.OUTPUT_SIZE,
                                 no_columns=ef_hp.NO_COLUMNS,
                                 b4=False, b2=True).to(device)

        # Train
        version = utils.short_id()
        train_function(predictions,
                       train_df,
                       test_df,
                       model,
                       MelanomaDataset,
                       folds,
                       device,
                       version=version)

        # Keep best model only
        utils.keep_best_model("saved_models")
        best_model_path = "saved_models/" + os.listdir("saved_models/")[0]
        dst = os.path.split(os.getcwd())[0] + "/ml/model/"
        shutil.copy(best_model_path, dst)

        print('Training complete.')
    except Exception as e:
        # Write out an error file. This will be returned as the failureReason in the
        # DescribeTrainingJob result.
        trc = traceback.format_exc()
        with open(os.path.join(output_path, 'failure'), 'w') as s:
            s.write('Exception during training: ' + str(e) + '\n' + trc)
        # Printing this causes the exception to be in the training job logs, as well.
        print('Exception during training: ' + str(e) + '\n' + trc, file=sys.stderr)
        # A non-zero exit code causes the training job to be marked as Failed.
        sys.exit(255)

if __name__ == '__main__':
    train()
    # A zero exit code causes the job to be marked a Succeeded.
    sys.exit(0)
